\documentclass[12pt,a4paper,english]{article}
\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage{babel,textcomp}
\usepackage{mathpazo}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{ dsfont }
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfig} 
\usepackage[colorlinks]{hyperref}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{textcomp}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{backgroundcolor=\color{lbcolor},tabsize=4,rulecolor=,language=python,basicstyle=\scriptsize,upquote=true,aboveskip={1.5\baselineskip},columns=fixed,numbers=left,showstringspaces=false,extendedchars=true,breaklines=true,
prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},frame=single,showtabs=false,showspaces=false,showstringspaces=false,identifierstyle=\ttfamily,keywordstyle=\color[rgb]{0,0,1},commentstyle=\color[rgb]{0.133,0.545,0.133},stringstyle=\color[rgb]{0.627,0.126,0.941},literate={å}{{\r a}}1 {Å}{{\r A}}1 {ø}{{\o}}1}

% Use for references
\usepackage[sort&compress,square,comma,numbers]{natbib}
\DeclareRobustCommand{\citeext}[1]{\citeauthor{#1}~\cite{#1}}

% Fix spacing in tables and figures
%\usepackage[belowskip=-8pt,aboveskip=5pt]{caption}
%\setlength{\intextsep}{10pt plus 2pt minus 2pt}

% Change the page layout
%\usepackage[showframe]{geometry}
\usepackage{layout}
\setlength{\hoffset}{-0.5in}  % Length left
%\setlength{\voffset}{-1.1in}  % Length on top
\setlength{\textwidth}{470pt}  % Width
%\setlength{\textheight}{720pt}  % Height
%\setlength{\footskip}{25pt}

\newcommand{\VEV}[1]{\langle#1\rangle}
\title{FYS-STK 4155 Project 1}
\date{}
\author{ Kristoffer Langstad\\ \textit{krilangs@uio.no}}

\begin{document}%layout
\maketitle
\begin{abstract}
	
\end{abstract}

\section{Introduction}
A hot topic today is what we call machine learning. We use machine learning to analyze data (often a huge amount). There are now many different methods for doing this, where some are better than others in different cases. A way to study machine learning is to look at regression analysis and resampling methods of given data sets, which is split into what we call training and test data. Then we can study the data dependence and the effect of different hyper-parameters.

We will in this project study three linear regression methods: the Ordinary Least Squares (OLS), Ridge regression and Lasso regression. In linear regression we will fit a polynomial to a given data set that we want to analyze. Linear regression let use look at relationships in data sets that are normally hidden under complexities and huge quantities of data. In our case we will first fit a polynomial to a two-dimensional function called Franke's function (which we will look at later). After implementing the regression methods we will use resampling techniques, like cross-validation, to perform a more thorough assessment of the models we have. Then we can also look at the Bias-Variance tradeoff as we look at a varying model complexity of the data set and models. Lastly we will use all the techniques we have mentioned to analyze real data terrain to see which model and degree of polynomial has the best fit.

In the methods section we look at the theory of the different models, techniques and the implementations of the algorithms we use in this project. We start by looking at the OLS of the Franke function where we use the Franke function, with and without an additional normally distributed stochastic noise, to produce a dataset to study with OLS regression. Then we want to fit a polynomial up to fifth order, and study the confidence interval of $\beta$ by computing the Mean Squared Error (MSE), $R^2$ score function and variance. Then we perform a resampling of the data where we split the data into training and test data. Then we construct the k-fold cross-validation algorithm and evaluate the MSE, $R^2$ score, variance and bias of the test data. For further studying machine learning, we study the bias and variance tradeoff as function of model complexity and number of data points. Now we have all the techniques we are using, so now we use the same techniques on the Ridge and Lasso method. For these two methods there is a hyperparameter dependence which we will also study. Then we want to introduce real terrain data to analyze. We use all three of the linear regression methods to evaluate which model fits the data best. In the Results section we look at the results and discuss all the methods and techniques used in this project. Here we look at data outputs and figures we get underway. Lastly in the Conclusion section we summarize the results and come up our thoughts on which model is the best fit for the terrain data.

\section{Theory}
\subsection{Linear regression}
In linear regression we want to fit a model to a set of data. For fitting a model to a set of data $\textbf{y}=[y_0, y_1,...,y_{n-1}]$ of length $n$ which are functions of some variables $\textbf{x}=[x_0, x_1,...,x_{n-1}]$. We can then define the prediction value $y$ as
\begin{equation}
\label{eq:pred_y}
y(x_i)=y_i=f(\cdot)+\epsilon_i=\sum_{n-1}^{i=0}\beta_jx_i^j+\epsilon_i,
\end{equation}
where $\beta$ are regression parameters and $\epsilon_i$ is a normally distributed error with mean zero and variance $\sigma^2$. The goal is to find the optimal regression parameters $\beta$. This can be done with different methods like OLS, Ridge and Lasso.

\subsection{Ordinary Least Square (OLS)}
In OLS we find the $\beta$ values that minimize the residual sum of squares (RSS), which here is a cost function dependent on the given data $y_i$ (data) and the parametrized values $\tilde{\textbf{y}}_i=\textbf{X}\hat{\beta}$ (model):
\begin{equation}
\label{eq:cost_OLS}
C(\hat{\beta})=\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\textbf{x}_{i*}\hat{\beta})^2=\frac{1}{n}\{(\textbf{y}-\textbf{X}\hat{\beta})^T(\textbf{y}-\textbf{X}\hat{\beta})\}
\end{equation} 
\textbf{X} is our design matrix which contains the predictors. Then to find the minimum of the cost function we set $\frac{\partial C}{\partial \hat{\beta}}=0$ and solve
\[\min_{\hat{\beta}\in \mathcal{R}^p}C(\hat{\beta}).\] This should give us the optimal $\beta$ values as
\begin{equation}
\label{eq:beta_OLS_opt}
\hat{\beta}_{\text{optimal}}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}
These $\beta$ values are the model values in the OLS model.

Taking the inverse of a matrix product (\textbf{X}$^T$\textbf{X})$^{-1}$ may be an expensive operation for bigger matrices. That is why we take use of the Singular Value Decomposition (SVD) to prevent taking the inverse of the product of twp big matrices. From the SVD theorem we can expand our design matrix as:
\[\textbf{X}=\textbf{U}\textbf{D}\textbf{V}^T\]
This is written in terms of the orthogonal transformation \textbf{U}, the diagonal matrix \textbf{D} containing the eigenvalues and \textbf{V}$^T$ which is also an orthogonal matrix. By inserting this into the OLS we get:
\begin{equation}
\label{eq:beta_SVD_OLS}
\tilde{\textbf{y}}=\textbf{X}\hat{\beta}_{\text{optimal}}=\textbf{U}\textbf{U}^T\textbf{y}
\end{equation}
This is now a more CPU friendly calculation of the OLS for finding the regression parameters $\hat{\beta}$.

\subsection{Ridge regression}
A problem with machine learning  and regression is overfitting of the data when the model complexity increases. The Ridge regression tries to take this into account by modification of the OLS method by introducing a complexity parameter $\lambda$, also called the ad-hoc fix for singularity of \textbf{X}$^T$\textbf{X} (\citet{wieringen2015}). The Ridge method shrinks the coefficients $\beta$ to go to zero as $\lambda$ increases. The complexity parameter is a parameter that controls the amount of shrinkage of the coefficients (\citet{hastie2009}). The cost function for Ridge can then be written as
\begin{equation}
\label{eq:cost_ridge}
C(\hat{\beta})=\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\textbf{x}_{i*}\hat{\beta})^2+\lambda\sum_{j=0}^{p-1}\beta_j^2=\frac{1}{n}\{(\textbf{y}-\textbf{X}\hat{\beta})^T(\textbf{y}-\textbf{X}\hat{\beta})\}+\lambda\hat{\beta}^T\hat{\beta}.
\end{equation}
The last term above is called the penalty term.
Then to find the optimal $\beta$ values we again take the minimum of the cost function to get
\begin{equation}
\label{eq:beta_opt_ridge}
\hat{\beta}_{\text{optimal}}=(\textbf{X}^T\textbf{X}+\lambda \textbf{I})^{-1}\textbf{X}^T\textbf{y},
\end{equation}
where \textbf{I} is the identity matrix with dimension $p\times p$ and $\lambda\geq0$. For orthonormal matrix (which we get when using the SVD in the OLS method) we get \textbf{X}$^T$\textbf{X}=\textbf{I} and
\begin{equation*}
\hat{\beta}^{\text{Ridge}}=\frac{1}{1+\lambda}\hat{\beta}^{\text{OLS}}
\end{equation*}
We see that for $\lambda=0$ we end up with the expression for the OLS method, and since $\lambda$ is defined to never be negative we get that the $\beta$ values are smaller than for the OLS. This will make the model less sensitive to data set changes and making it less likely to overfit the training data. The model now should give a decrease in variance as the $\lambda$ increases, but then in return give an increase in the bias.

\subsection{Lasso regression}
The Lasso method is similar to the Ridge method in that case it has a complexity parameter $\lambda$, but now we change the penalty term in the cost function equation \ref{eq:cost_ridge}:
\begin{equation}
\label{eq:cost_lasso}
C(\hat{\beta})=\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\textbf{x}_{i*}\hat{\beta})^2+\lambda\sum_{j=0}^{p-1}|\beta_j|
\end{equation}
A difference with the Lasso is that it uses methods like gradient descent to find the $\hat{\beta}$ that minimize the cost function. For the Lasso there is no explicit expression for the model parameters like for the OLS and Ridge methods. In this project we will just use Scikit-Learn's Lasso function, instead of making our own which can be difficult. Like the Ridge method the task is to make the $\beta$ values go to zero, but in a slightly different way. So the variance should decrease while the bias should increase with increasing $\lambda$ also here.

\subsection{Confidence interval}
For the confidence interval of the parameters $\beta$ we need to calculate the variance of the models. The variance of the OLS model can be calculated as
\begin{equation}
\label{eq:var_ols}
Var(\hat{\beta})=\sigma^2(\textbf{X}^T\textbf{X})^{-1},
\end{equation}
where $\sigma^2$ is the variance of the normally distributed error $\epsilon_i$ in equation \ref{eq:pred_y}. This variance can be calculated as
\begin{equation}
\label{eq:sigma}
\sigma^2 = \frac{1}{n-p-1}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2.
\end{equation}
In \citet{hastie2009}\footnote{Chapter 3 - equation (3.14)} it is shown that the confidence interval of $\beta_j$ can be calculated as 
\begin{equation}
\label{eq:conf_int}
(\hat{\beta}_j-z^{(1-\alpha)}v_j^{\frac{1}{2}}\hat{\sigma}, \hat{\beta}_j+z^{(1-\alpha)}v_j^{\frac{1}{2}}\hat{\sigma})
\end{equation}
For a 95\% confidence interval we have $\alpha=0.05$ such that we get $z^{(1-0.05)}=1.96$. $v_j$ is the $j$-th diagonal element of (\textbf{X}$^T$\textbf{X})$^{-1}$.

\subsection{Error evaluation}
Two very useful tools for evaluating the error in predicted values are the Mean Squared Error (MSE) and the $R^2$ score function. The MSE is calculated as 
\begin{equation}
\label{eq:MSE}
MSE(\textbf{y}, \tilde{\textbf{y}})=\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y})^2,
\end{equation}
and the $R^2$ score function is calculated as
\begin{equation}
\label{eq:R2_score}
R^2(\textbf{y},\tilde{\textbf{y}})=1-\frac{\sum_{i=0}^{n-1}(y_i-\tilde{y})^2}{\sum_{i=0}^{n-1}(y_i-\bar{y})^2}
\end{equation}
with the mean value of the data $y$ as
\[\bar{y}=\frac{1}{n}\sum_{i=0}^{n-1}y_i.\]
$\textbf{y}_i$ is the predicted value of the $i$-th sample and $y_i$ is the corresponding true value of the data. For the MSE we want the error to be as small as possible, close to zero. For the $R^2$ score, the best value we can get is 1. This can be described as how much variance in the data is accounted for by the model. For a score of 1 means that the prediction fits the data perfect. The score can lie in the interval $(-\infty,1]$, meaning that the score can be negative. This can be interpreted such that the mean is a better fit than the model used.

\subsection{Bias-variance tradeoff}
To find the regression parameters $\hat{\beta}$ we have to look at the cost function again:
\begin{equation}
\label{eq:cost_func}
C(\textbf{X},\hat{\beta})=\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2= \mathds{E}[(\textbf{y}-\tilde{\textbf{y}})^2]
\end{equation}
This can be rewritten as 
\begin{equation}
\label{eq:bias_var}
\mathds{E}[(\textbf{y}-\tilde{\textbf{y}})^2]= \frac{1}{n}\sum_{i}(f_i-\mathds{E}[\tilde{\textbf{y}}])^2+ \frac{1}{n}\sum_{i}(\tilde{y}_i-\mathds{E}[\tilde{\textbf{y}}])^2+\sigma^2.
\end{equation}
The full calculation can be found in appendix \ref{sect:bias_var_calc}.

From the rewritten cost function we now have three terms in the equation above. The first term is the bias squared
\begin{equation}
\label{eq_Bias}
Bias=\frac{1}{n}\sum_{i=0}^{n-1}(f_i-\mathds{E}[\tilde{\textbf{y}}])^2.
\end{equation}
The second term is the variance of the model
\begin{equation}
\label{eq:var}
Var=\frac{1}{n}\sum_{i}(\tilde{y}_i-\mathds{E}[\tilde{\textbf{y}}])^2
\end{equation}
The last term $\sigma^2$ is the irreducible error of a normally distributed noise.

Here we can see that if the variance is decreased then the bias is increased, and the opposite for increased bias. For increasing complexity/degree of polynomial, we would expect the variance to increase and the bias decrease. This is the bias-variance tradeoff. The bias of the model can be seen as the measure of the tendency the model has to either overestimate or underestimate the predictions. When we have many $\beta$ parameters then we may get something called overfitting. This is when we get too high variance/low bias of the model. This comes from that the model may start to fit the noise as well a the training data. One precaution is to split the data into training and test data. This is to avoid the noise being take with the test data. Another method is to add a term to reduce the variance even though that will increase the bias. Often the increase in the bias is so small, such that the tradeoff we get is acceptable. On the other end we can have underfitting for high bias and low variance. This happens when the complexity is very small so that the model can't pick up features of the data set.

\subsection{Resampling}
We don't always have enough data to be able to fit our model. To solve this problem we can use resampling of the data. A way to split our data set is to use cross-validation techniques like the k-fold cross-validation resampling. This splits the data set into training and test data. The main idea of resampling techniques is to gain more data by drawing arbitrary samples from the training and refitting a model on each sample set. These new sets can then be evaluated to see how well they correspond to each other. The k-fold splits the data into $k$ sets/folds of data. Normally this is between 5 and 10 folds, while normally we split the data into 70-80\% training data and the remaining test data. One of the folds is set as a validation set, while the $k-1$ other folds are trained on our model for given hyperparameters. Then we test our model on the validation set. This is repeated until all the folds have been validated. Then we calculate the mean of the MSE for each iteration from the test data. This is then used as the expected prediction error. We can also calculate the $R^2$ score, variance and bias.

\subsection{Data}
The first data set in this project is the data we get from fitting polynomials to the two-dimensional Franke function:
\begin{align}
\label{eq:Franke_func}
f(x,y)=&\frac{3}{4}\exp\left(-\frac{(9x-2)^2}{4}-\frac{(9y-2)^2}{4}\right)+
\frac{3}{4}\exp\left(-\frac{(9x+1)^2}{49}-\frac{(9y+1)^2}{10}\right)\\ 
+& \frac{1}{2}\exp\left(-\frac{(9x-7)^2}{4}-\frac{(9y-3)^2}{4}\right)- \frac{1}{5}\exp\left(-(9x-4)^2-(9y-7)^2\right) \nonumber
\end{align}
This function is defined for $x,y\in[0,1]$, and is a widely used function for testing various interpolation and fitting algorithms. With this function we also add a normally distributed noise.

The second set is (real) digital terrain data that we download from the
U.S. Geological Survey EarthExplorer website \cite{earthexplorer}. The chosen digital terrain data we use is of Hardangervidda in Norway.

\section{Methods}






\section{Results}
\section{Conclussion}

\appendix
\section{Appendix}
\label{sect:appendix}
Link to GitHub repository:\\

\section{Appendix}
\label{sect:bias_var_calc}
The full calculation of the rewritten cost function from equation \ref{eq:cost_func} to the bias-variance tradeoff equation \ref{eq:bias_var}:\\
We start by using that $\textbf{y}=f(\textbf{x})+\hat{\epsilon}$ and $\tilde{\textbf{y}}=\textbf{X}\hat{\beta}$.
\begin{align*}
\mathds{E}[(\textbf{y}-\tilde{\textbf{y}})^2]&=\mathds{E}[(\textbf{f}+\hat{\epsilon}-\tilde{\textbf{y}})^2]= \mathds{E}[(\textbf{f}+\hat{\epsilon}-\tilde{\textbf{y}}+\mathds{E}[\tilde{\textbf{y}}]-\mathds{E}[\tilde{\textbf{y}}])^2]\\
&=\mathds{E}[(\textbf{f}^2-2\textbf{f}\mathds{E}[\tilde{\textbf{y}}]+\mathds{E}[\tilde{\textbf{y}}]^2)+(\tilde{\textbf{y}}^2-2\tilde{\textbf{y}}\mathds{E}[\tilde{\textbf{y}}]+\mathds{E}[\tilde{\textbf{y}}]^2)+\mathds{E}[\hat{\epsilon}^2]]
\end{align*}
Here we have used that the expectation value of $\hat{\epsilon}$ is $\mathds{E}[\hat{\epsilon}]=0$, and that the expectation value of $\tilde{\textbf{y}}$ is $\mathds{E}[\tilde{\textbf{y}}]=\textbf{X}\hat{\beta}$ \cite{hastie2009}\footnote{Chapter 7.3 - equation (7.9)}. Then we split the equation such that we get:
\begin{align*}
\mathds{E}[(\textbf{y}-\tilde{\textbf{y}})^2]=\mathds{E}[(\textbf{f}-\mathds{E}[\tilde{\textbf{y}}])^2]+\mathds{E}[(\tilde{\textbf{y}}-\mathds{E}[\tilde{\textbf{y}}])^2]-\mathds{E}[\hat{\epsilon}^2]
\end{align*}
Now we use that the expectation value of $\hat{\epsilon}^2$ is $\mathds{E}[\hat{\epsilon}^2]=Var[\textbf{y}]=\sigma^2$ since $\mu=0$. The other expectation values can be discretized in terms of sums as:
\begin{equation*}
\mathds{E}[(\textbf{y}-\tilde{\textbf{y}})^2]=\frac{1}{n}\sum_{i}(f_i-\mathds{E}[\tilde{\textbf{y}}])^2+ \frac{1}{n}\sum_{i}(\tilde{y}_i-\mathds{E}[\tilde{\textbf{y}}])^2+\sigma^2
\end{equation*}

\bibliographystyle{plainnat}
\bibliography{myrefs}
\end{document}