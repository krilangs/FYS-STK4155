\documentclass[12pt,a4paper,english]{article}
\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage{babel,textcomp}
\usepackage{mathpazo}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{ dsfont }
\usepackage{listings}
\usepackage{graphicx}
\usepackage{ mathrsfs }
\usepackage{float}
\usepackage{subfig} 
\usepackage[colorlinks]{hyperref}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{textcomp}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{backgroundcolor=\color{lbcolor},tabsize=4,rulecolor=,language=python,basicstyle=\scriptsize,upquote=true,aboveskip={1.5\baselineskip},columns=fixed,numbers=left,showstringspaces=false,extendedchars=true,breaklines=true,
prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},frame=single,showtabs=false,showspaces=false,showstringspaces=false,identifierstyle=\ttfamily,keywordstyle=\color[rgb]{0,0,1},commentstyle=\color[rgb]{0.133,0.545,0.133},stringstyle=\color[rgb]{0.627,0.126,0.941},literate={å}{{\r a}}1 {Å}{{\r A}}1 {ø}{{\o}}1}

% Use for references
\usepackage[square,comma,numbers]{natbib}
%\DeclareRobustCommand{\citeext}[1]{\citeauthor{#1}~\cite{#1}}

% Fix spacing in tables and figures
%\usepackage[belowskip=0pt,aboveskip=5pt]{caption}
%\setlength{\intextsep}{10pt plus 2pt minus 2pt}

% Change the page layout
%\usepackage[showframe]{geometry}
%\usepackage{layout}
\setlength{\hoffset}{-0.5in}  % Length left
\setlength{\voffset}{-0.7in}  % Length on top
\setlength{\textwidth}{460pt}  % Width /597pt
\setlength{\textheight}{660pt}  % Height /845pt
%\setlength{\footskip}{25pt}

\newcommand{\VEV}[1]{\langle#1\rangle}
\title{FYS-STK4155 - Project 3}
\date{}
\author{ Kristoffer Langstad \footnote{\url{https://github.com/krilangs/FYS-STK4155}}\\ \textit{krilangs@uio.no}}

\begin{document}%\layout
\maketitle
\begin{abstract}
	...
\end{abstract}
\section{Introduction}
\label{sect:Introduction}
Machine learning is a more and more used method for classification problems. One application of machine learning is to create filters that decreases the human dependency for classification. Normally a human is needed to go through data and decide what the classification of an object is. With machine learning the goal is to make this as automated as possible by using suited algorithms and methods. We will look at several methods for classification in this project.

In this project we will study a binary classification problem involving the prediction of pulsar star candidates collected during the HTRU survey from the UCI Machine Learning Repository \cite{UCI}. This data set, called HTRU2, has previously been studied in a scientific paper by \citet{pulsar_art} for finding good filters for classification of pulsars. So some of the results in this paper is used to compare with the results we get. We will use several classification methods from the Scikit-Learn library in Python for classifying the pulsar data, to see which method(s) is the best for classifying these pulsar data. We will explore different approaches from decision trees to random forests, different boosting methods and a couple of other classification methods. For the analysis part we will look at among others the accuracy score, Cohen Kappa score, mean squared error (MSE), confusion matrix and so on.

First we will look at the HRTU2 data set to be evaluated, what it contains and information about the data set. Then in the theory section we will look at the theory of the different classification methods and analysis methods to be used. In the methods section we explain the implementation of the algorithms in Python. Here we first download and read in the data to a Python program. With this program we will look at the data a bit more by looking at the correlation and the information gain
of the data features. Then we split the data into training and test data to be used in the various algorithms. For these algorithms we test to find the best input parameters. In the results section we will present and discuss the classification results, and compare them with the scientific report. Lastly, we give a critical evaluation of the methods on which is the best method on the pulsar data set.

\section{Data}
\label{sect:Data}
The HRTU2 data set we are analyzing is downloaded from the UCI Machine Learning Repository \cite{UCI} as a CSV file. The data file contains feature data about possible pulsar candidates collected during the High Time Resolution Universe (HTRU) survey, and are classified as pulsars (positive, 1) or non-pulsars (negative, 0). 

Pulsar stars are Neutron stars that rotate with such velocity that they emit radio emission that we can detect on Earth. These stars are often very rare, and are interesting to us in the matter of probes of space-time, interstellar-medium and states of matter \cite{UCI}. The beam the pulsars emit when rotating are different from pulsar to pulsar, and they may even vary a little for each rotation. So to find a potential signal candidate, the signals are averaged over many rotations of the pulsars. In most cases the detected signals are caused by noise and radio frequency interference (RFI), and not signals from real pulsars.

The data set contains a total of 17 898 number of instances (161 082 data points), where 1 639 are real pulsar examples and 16 259 are spurious examples caused by noise/RFI. So the real pulsar data examples are a minority positive class. All the data have been checked by human annotators. Each candidate is stored in a separate row and contains 8 continuous variables and the class label (0 or 1):
\begin{enumerate}
	\item Mean of the integrated profile.
	\item Standard deviation of the integrated profile.
	\item Excess kurtosis of the integrated profile.
	\item Skewness of the integrated profile.
	\item Mean of the DM-SNR curve.
	\item Standard deviation of the DM-SNR curve.
	\item Excess kurtosis of the DM-SNR curve.
	\item Skewness of the DM-SNR curve.
	\item Class
\end{enumerate}
The first four variables are simple statistics from the integrated pulse profile. They describe the longitude-resolved version of the signal that has been averaged in both time and frequency. The next four are obtained from the DM-SNR curve, and contains the same type of information. Their mathematical definitions can be seen in Table 4 in the scientific paper by \citet{pulsar_art}. In this scientific paper (section 5) there are also more thorough explanations about the pulsar data and how they are produced.

The 8 continuous feature variables are set as the design matrix \textbf{X}, and the class variable are the target vector \textbf{y}. The data set looks to be complete without any NAN values.

\section{Theory}
\label{sect:Theory}
\subsection{Classification Methods}
\label{subsect:class_methods}
.........
\subsubsection{Logistic Regression}
\label{subsubsect:LR}
\subsubsection{Support Vector Machine}
\label{subsubsect:SVM}
\subsubsection{Decision Tree}
\label{subsubsect:Tree}
\subsubsection{Bagging}
\label{subsubsect:Bag}
\subsubsection{Random Forest}
\label{subsubsect:Forest}
\subsubsection{Voting Classifier}
\label{subsubsect:Vote}
\subsubsection{AdaBoost}
\label{subsubsect:Ada}
\subsubsection{Gradient Boosting}
\label{subsubsect:Grad}
\subsubsection{Extreme Gradient Boosting}
\label{subsubsect:XGB}

\subsection{Evaluation Methods}
\label{subsect:eval_methods}
Since we have a binary classification problem (dichotomous target variable), we can compute the point-biserial correlation coefficient $r_{pb}$ between the data features (sect. 5.1.3 in \citet{pulsar_art}). This is a measurement of the linear correlation between the continuous features and the binary target, which yields values between -1 and 1. This is calculated as 
\begin{equation}
\label{eq:point_biserial}
r_{pb}=\frac{\overline{x}_1-\overline{x}_2}{\sigma}\cdot\sqrt{\frac{n_2\cdot n_1}{n(n-1)}},
\end{equation}
where $n$ is the total number of samples, $\overline{x}_i$ is the mean value of a feature group and $\sigma$ is the sample standard deviation. This is calculated with the \textit{pointbiserialr} function from the Scipy.stats library in Python on the design matrix \textbf{X} and target \textbf{y}. This is calculated for each of the 8 features.

For more information on the correlation in the data set, we can look at the entropy and information gain. The entropy is calculated as
\begin{equation*}
H(x_i)=-\sum_{j\in x_i}P(j)\log_2P(j),
\end{equation*}
where $j$ corresponds to each value that a feature group $x_i$ can take and $P(j)$ is the probability of $j$ occurring. The conditional entropy of the feature $x_i$ given target \textbf{y} is 
\begin{equation*}
H(x_i|\textbf{y})=-\sum_{y\in\textbf{y}}P(y)\sum_{j\in x_i}P(j|y)\log_2P(j|y).
\end{equation*} 
With these definitions we define the information gain (mutual information) for a given feature as the difference between the total outcome entropy and the entropy of the given feature as
\begin{equation}
\label{eq:inf_gain}
I(x_i:\textbf{y})=H(x_i)-H(x_i|\textbf{y}).
\end{equation}
The information gain is a measure of the correlation between a feature and the target which detects non-linearities. This is calculated using the \textit{mutual\_info\_classif} function from the Scikit-Learn library.

For classification we measure the performance of the model with the accuracy score which is the number of correct predictions divided by the total number of predictions:
\begin{equation}
\label{eq:accuracy}
\text{Accuracy}=\frac{\sum_{i=1}^{n}I(t_i=y_i)}{n}
\end{equation}
Here $n$ is the total number of predictions, $t_i$ is the predicted target, $y_i$ is the class target and $I$ is an indicator function as
\[I=\begin{cases}
1, \text{ if } t_i=y_i\\
0, \text{ if } t_i\neq y_i
\end{cases}.\]
The optimal accuracy score is 1, which means that the prediction fits the data perfect. With the \textit{cross\_val\_score} function in the Scikit-Learn library we also calculate the test cross-validation score with 5 folds with the test data for the chosen classifier.

Another score function is the Cohen Kappa score, which takes into account the accuracy that would have happened through random predictions. It measures how well the classifier actually performs, and the best score is 1 as for the accuracy score. The Kappa score is calculated as (\cite{kappa})
\begin{equation}
\label{eq:kappa}
\kappa=\frac{\text{Observed Accuracy} - \text{Expected Accuracy}}{1-\text{Expected Accuracy}}=\frac{p_a-p_e}{1-p_e}.
\end{equation}

For evaluating the error in the predicted values we also calculate the mean squared error (MSE) as
\begin{equation}
\label{eq:mse}
MSE(\textbf{y},\tilde{\textbf{y}})=\frac{1}{n}\sum_{i=1}^{n}(y_i-\tilde{y}_i)^2,
\end{equation}
where $\tilde{y}_i$ is the predicted value and $y_i$ is the true value of the variables being predicted of the $i$-th sample.

For more error evaluation we also look at the variance of the predicted values, which is a measure on how far the predictions are spread out from their average value. It is also the represented as the square of the standard deviation. The variance is calculated as
\begin{equation}
\label{eq:var}
Var(\tilde{\textbf{y}})=\frac{1}{n}\sum_{i=1}^{n}(\tilde{y}_i-\frac{1}{n}\sum_{i=1}^{n}\tilde{y}_i)^2.
\end{equation}

Then we have the bias error, which is the difference between the expected value and the true value. The bias squared is calculated as
\begin{equation}
\label{eq:bias}
Bias(\textbf{y},\tilde{\textbf{y}})=\frac{1}{n}\sum_{i=1}^{n}(y_i-\frac{1}{n}\sum_{i=1}^{n}\tilde{y}_i)^2.
\end{equation}

With the \textit{classification\_report} function from the Scikit-Learn library we can compute a classification report. This classification report contains various averages for the two target classes 0 and 1 (Table 10 in \citet{pulsar_art}):
\begin{enumerate}
	\item Precision: Fraction of retrieved instances that are positive: \[\frac{TP}{TP+FP}\]
	\item Recall: Fraction of positive instances that are retrieved:
	\[\frac{TP}{TP+FN}\]
	\item F1-score: Measure of accuracy that considers both precision and recall:
	\[2\times\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}\]
	\item Support: Predictive positive rate=fraction of positively predicted examples:
	\[\frac{TP+FP}{N}\]
	\item Accuracy: The accuracy score of the F1-score only (in our case).
	\item Macro avg: Average of the unweighted mean per label.
	\item Weighted avg: Average of the support-weighted mean per label.
\end{enumerate}
Here we use the prediction results to calculate the precision and recall. They are given as:
\begin{enumerate}
	\item[] Positive (P) - Observation is positive.
	\item[] Negative (N) - Observation is negative.
	\item[] True Positive (TP) - Observation is positive, and prediction is positive.
	\item[] True Negative (TN) - Observation is negative, and prediction is negative.
	\item[] False Positive (FP) - Observation is negative, and prediction is positive.
	\item[] False Negative (FN) - Observation is positive, and prediction is negative.
\end{enumerate}

These outcomes are also used in the confusion matrix for binary outcomes. The confusion matrix will look like Figure \ref{fig:conf_mat}. The values in the confusion matrix are normalized such that the total value of the rows equals 1. The optimal values are TN=1, FP=0, FN=0 and TP=1. This means that all the predictions are correct.

\begin{figure}[h!]
	\centering\includegraphics[width=0.4\linewidth]{Conf_mat.png}
	\caption{Confusion matrix describing the outcomes of the pulsars.\label{fig:conf_mat}}
\end{figure} 

Another plot we can compute with these prediction results, is the Receiver Operating Characteristic (ROC) curve. This plots the true positive rate as a function of the false positive rate for the class 0, class 1, micro-average and macro-average for the classifier. The earlier the true positive rate reaches 1.0, the better the accuracy of the classifier.

By plotting the cumulative gain of the class targets as functions of the percentage of samples, we can see how effective the classifier is at predicting the class targets as the percentage of the samples vary.

For the XGBoost we can look at the feature importance when building the trees. This counts the number of times a feature is split across all boosting trees. With the \textit{plot\_importance} function in the XGBoost library, we get a bar graph over how many times each feature appear in the trees.


\section{Methods}
\label{sect:Methods}
The programming is done in Python 3.7 in the program \textit{classif.py} found at the GitHub repository (\ref{sect:Appendix}), among with the pulsar data file and produced figures.

\subsection{Analyzing the Data}
\label{subsect:analysis}
We start by reading in the downloaded CSV file \textit{pulsar\_stars.csv} of the pulsar candidates in the program. Then we rename the feature variables to be shorter. By using the panda-library, we do an analysis of the data set to make sure that the data file is complete and read in correct. We also check the correlation between the data features by plotting a correlation heatmap using the seaborn-library. Next, we create the design matrix \textbf{X} and target vector \textbf{y} as mentioned in the Data section (\ref{sect:Data}). As in the scientific report \cite{pulsar_art}, we calculate the point-biserial correlation coefficient of the data features with the design matrix and target vector. The design matrix and target vector are then split into training and test data with 27\% test data. Then we scale the design training and test data with respect to the training data. With the training data we calculate the information gain of the features.

\subsection{Classification}
\label{subsect:classif}
Then we implement all the classification methods we explained in the Theory section (\ref{sect:Theory}) into our program using the Scikit-Learn xgboost libraries. This includes (input parameters that are not covered, uses the default parameters):
\begin{enumerate}
	\item Logistic Regression (LR): Where we use the \textit{liblinear}-solver.
	\item Support Vector Machine (SVM): Where we use probability=True.
	\item Decision Tree: Where we set the maximum depth of the tree to 6. Here we also plot a decision tree for visualization.
	\item Bagging: Where we use bagging on the decision tree with bootstrap, 500 number of estimators in the ensemble, 200 number of samples to train on each base estimator and use all processors to run the jobs in parallel.
	\item Random Forest: With 500 number of estimators.
	\item Voting classifier: Use both hard and soft voting separately on all of the mentioned classifier methods mentioned above together (1-5) and use all processors to run the jobs in parallel.
	\item AdaBoost: Use AdaBoost on the decision tree with 200 number of estimators, learning rate of 0.5 and the "SAMME.R" real boosting algorithm.
	\item Gradient boosting: Where we use the exponential loss function to be optimized and 500 number of estimators.
	\item XGBoost: Where we use the "multi:softprob" learning objective with 2 classes, gbtree booster, 200 number of estimators, 0.1 learning rate, 10 threads to run in parallel and set maximum depth of the tree to 5. Here we also plot the visualization of the first decision tree, and plot the feature importance in the trees.
	\item Voting classifier: Where we use the soft voting classifier on all the used classification methods (except the other voting classifiers).
\end{enumerate}

For each of the classification methods above, we do the same evaluations of the pulsar data set. We fit the classification methods to the training data. Then we use the test data to make a prediction of the class labels. With this prediction we calculate the MSE, variance, bias, accuracy score and the Cohen Kappa score. We also make a classification report containing various averages for the classification methods. Then we plot the confusion matrix of the predicted results of the pulsars. With the test data we calculate the cross-validation score with 5 folds. Then we plot the ROC curve for the classification methods for the sensitivity as function of the 100-specificity. For the hard voting classification we use the prediction, while for the others we use the probability estimation of the test data to make the ROC curve plot. Lastly we use the probability estimate to plot the cumulative gain for the class targets as functions of the percentage of the samples.

\section{Results}
\label{sect:Results}
\section{Conclusion}
\label{sect:Conclusion}

\appendix
\section{Appendix}
\label{sect:Appendix}
Link to GitHub repository:\\
\url{https://github.com/krilangs/FYS-STK4155/tree/master/Project3}

\bibliographystyle{plainnat}
\bibliography{myrefs}
\end{document}
